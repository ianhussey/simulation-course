
# OG by chatGPT

with some corrections

```{r}

# Define the model with specified factor loadings for each item
model <- '
# Latent variable
factor =~ 0.60*item1 + 0.65*item2 + 0.70*item3 + 0.75*item4 + 0.80*item5 + 0.85*item6 + 0.90*item7
'

library(lavaan)
library(psych)

# Simulation parameters
n_iterations <- 1000 # Number of simulations
n_indicators <- 7 # ensure this matches the model specified above
correct_removals <- 0 # To track correct identification of the worst item
n_participants <- 50
set.seed(42) # For reproducibility

# Simulation loop
for (i in 1:n_iterations) {
  # Generate data using the specified model
  data <- simulateData(model, sample.nobs = n_participants)
  
  # Compute Cronbach's alpha for the full scale
  initial_alpha <- suppressMessages(alpha(data)$total$raw_alpha)
  
  # Determine which item's removal leads to the best alpha
  alphas_if_removed <- suppressMessages(sapply(1:n_indicators, function(item) alpha(data[, -item], warnings = FALSE)$total$raw_alpha))
  item_to_remove <- which.max(alphas_if_removed)
  
  # Check if the item with the lowest factor loading is removed
  if (item_to_remove == 1) { # Since item1 has the lowest factor loading
    correct_removals <- correct_removals + 1
  }
}

# Calculate and report the percentage of correct removals
percentage_correct_removals <- (correct_removals / n_iterations) * 100
cat("Percentage of cases where the item with the worst factor loading was removed:", percentage_correct_removals, "%\n")


```

# Save the results in a vector instead

```{r}

# Define the model with specified factor loadings for each item
model <- '
# Latent variable
factor =~ 0.40*item1 + 0.65*item2 + 0.70*item3 + 0.75*item4 + 0.80*item5 + 0.85*item6 + 0.90*item7
'

library(lavaan)
library(psych)


# Simulation parameters
n_iterations <- 100 # Number of simulation iterations
n_indicators <- 7 # ensure this matches the model specified above
n_participants <- 50
set.seed(42) # For reproducibility

correct_removals <- numeric(n_iterations) # To track correct identification of the worst item

# Simulation loop
for (i in 1:n_iterations) {
  # Generate data using the specified model
  data <- simulateData(model, sample.nobs = n_participants)
  
  # Compute Cronbach's alpha for the full scale
  initial_alpha <- suppressMessages(alpha(data, warnings = FALSE)$total$raw_alpha)
  
  # Determine which item's removal leads to the best alpha
  alphas_if_removed <- suppressMessages(sapply(1:n_indicators, function(item) alpha(data[, -item], warnings = FALSE)$total$raw_alpha))
  item_to_remove <- which.max(alphas_if_removed)
  
  # Check if the item with the lowest factor loading is removed
  correct_removals[i] <- item_to_remove == 1
  
}

# Calculate and report the percentage of correct removals
paste0((sum(correct_removals) / n_iterations)*100, "% correct item drops")

```

# Save the change in alpha instead of the item selection

```{r}

library(lavaan)  # For simulation of data
library(psych)   # For Cronbach's alpha calculation

# Simulation parameters
n_iterations <- 100 # Number of simulation iterations
n_indicators <- 7 # ensure this matches the model specified above
#n_indicators <- 12 # ensure this matches the model specified above
n_participants <- 50
set.seed(42) # For reproducibility

alpha_changes <- numeric(n_iterations) # To track changes in alpha after removal

# Define the model with specified factor loadings for each item
# NB all equal loadings

model <- '
# Latent variable, alpha = .70 with this number of items
factor =~ 0.58*item1 + 0.58*item2 + 0.58*item3 + 0.58*item4 + 0.58*item5 + 0.58*item6 + 0.58*item7
'

# model <- '
# # Latent variable
# factor =~ 0.60*item1 + 0.60*item2 + 0.60*item3 + 0.60*item4 + 0.60*item5 + 0.60*item6 + 0.60*item7 + 0.60*item8 + 0.60*item9 + 0.60*item10 + 0.60*item11 + 0.60*item12
# '

# model <- '
# # Latent variable
# factor =~ 0.45*item1 + 0.50*item2 + 0.55*item3 + 0.60*item4 + 0.65*item5 + 0.70*item6 + 0.75*item7
# '

# model <- '
# # Latent variable
# factor =~ 0.70*item1 + 0.70*item2 + 0.70*item3 + 0.70*item4 + 0.70*item5 + 0.70*item6 + 0.70*item7 
# '
# 
# model <- '
# # Latent variable
# factor =~ 0.80*item1 + 0.80*item2 + 0.80*item3 + 0.80*item4 + 0.80*item5 + 0.80*item6 + 0.80*item7 
# '

large_sample_alpha <- alpha(simulateData(model, sample.nobs = 1000000))$total$raw_alpha

# Simulation loop
for (i in 1:n_iterations) {
  # Generate data using the specified model
  data <- simulateData(model, sample.nobs = n_participants)
  
  # Compute Cronbach's alpha for the full scale
  initial_alpha <- suppressMessages(alpha(data, warnings = FALSE)$total$raw_alpha)
  
  # Determine which item's removal leads to the highest alpha
  alphas_if_removed <- suppressMessages(sapply(1:n_indicators, function(item) alpha(data[, -item], warnings = FALSE)$total$raw_alpha))

  # Calculate the change in alpha
  best_alpha_if_removed <- max(alphas_if_removed)  # Best alpha after removing the identified item
  alpha_change <- best_alpha_if_removed - initial_alpha  # Change in alpha
  
  alpha_changes[i] <- alpha_change  # Store the change in alpha for this iteration
}

large_sample_alpha

# Calculate and report the average change in alpha after removing the item
# hist(alpha_changes)
# median(alpha_changes)

hist(alpha_changes[alpha_changes > 0])
median(alpha_changes[alpha_changes > 0])

```

# Reproduce Kopalle & Lehman 1996

partial reproduction, different item dropping strategy

```{r}

# generate lavaan syntax for a latent variable specification model where each factor loading (lambda_i) is drawn from a population model that is normally distributed (i.e., following lambda_mu and lambda_sigma)

generate_model_normal_distribution <- function(k_indicators, lambda_mu, lambda_sigma) {
  # Ensure that k_indicators is an integer and coefficient is numeric
  if (!is.numeric(k_indicators) || !is.numeric(lambda_mu) || !is.numeric(lambda_sigma)) {
    stop("k_indicators, lambda_mu, and lambda_sigma must be numeric")
  }
  
  # Generate each item string
  itemStrings <- sapply(1:k_indicators, function(i) {
    paste(format(rnorm(n = 1, mean = lambda_mu, sd = lambda_sigma), nsmall = 2), "*item", i, sep = "")
  })
  
  # Concatenate all item strings with " + " separator
  resultString <- paste("factor =~", paste(itemStrings, collapse = " + "))
  
  return(resultString)
}

# Example usage
generate_model_normal_distribution(k_indicators = 5, lambda_mu = 0.4, lambda_sigma = 0.058)

```

```{r}

library(lavaan)  # For simulation of data
library(psych)   # For Cronbach's alpha calculation

set.seed(42) 

# Total number of items
k_indicators <- 20 # 10, 20, 30

# Sample size
n_participants <- 3*k_indicators # 3N, 10N

# # Number of items chosen
# k_indicators_chosen <- 5 # k = 3, 5, 10

# True factor loading
lambda_mu <- 0.5 # 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.
lambda_sigma <- 0.058

# Simulation parameters
n_iterations <- 100 # Number of simulation iterations

#large_sample_alpha <- alpha(simulateData(generate_model_normal_distribution(k_indicators = 5, lambda_mu = 0.4, lambda_sigma = 0.058), sample.nobs = 1000000))$total$raw_alpha

# check this function works outside the loop
generate_model_normal_distribution(k_indicators = k_indicators, 
                                   lambda_mu = lambda_mu, 
                                   lambda_sigma = lambda_sigma)



alpha_changes <- numeric(n_iterations) # To track changes in alpha after removal

# Simulation loop
for (i in 1:n_iterations) {
  # Generate data using the specified model
  data <- simulateData(generate_model_normal_distribution(k_indicators = k_indicators, 
                                                          lambda_mu = lambda_mu, 
                                                          lambda_sigma = lambda_sigma), 
                       sample.nobs = n_participants)
  
  # Compute Cronbach's alpha for the full scale
  initial_alpha <- suppressMessages(alpha(data, warnings = FALSE)$total$raw_alpha)
  
  # Determine which item's removal leads to the highest alpha
  alphas_if_removed <- suppressMessages(sapply(1:k_indicators, function(item) alpha(data[, -item], warnings = FALSE)$total$raw_alpha))

  # Calculate the change in alpha
  best_alpha_if_removed <- max(alphas_if_removed)  # Best alpha after removing the identified item
  alpha_change <- best_alpha_if_removed - initial_alpha  # Change in alpha
  
  # Store the change in alpha for this iteration
  alpha_changes[i] <- alpha_change  
}

# results
hist(alpha_changes)
hist(alpha_changes[alpha_changes > 0])
median(alpha_changes[alpha_changes > 0])

```



# Reproduce Kopalle & Lehman 1996

partial reproduction, different item dropping strategy

drop arbitrary number of items

```{r}

library(lavaan)  # For simulation of data
library(psych)   # For Cronbach's alpha calculation

set.seed(42) 

# Total number of items
k_indicators <- 20 # for all of 10, 20, 30

# Sample size
n_participants <- 3*k_indicators # for all of 3*k_indicators, 10*k_indicators

# # Number of items chosen
k_indicators_retained <- 3 # also for 3, 5, 10

# True factor loading
lambda_mu <- 0.5 # for all of 0.3, 0.4, 0.5, 0.6, 0.7, 0.8
lambda_sigma <- 0.058

# Number of simulation iterations
n_iterations <- 10 

#large_sample_alpha <- alpha(simulateData(generate_model_normal_distribution(k_indicators = 5, lambda_mu = 0.4, lambda_sigma = 0.058), sample.nobs = 1000000))$total$raw_alpha

# generate lavaan syntax for a latent variable specification model where each factor loading (lambda_i) is drawn from a population model that is normally distributed (i.e., following lambda_mu and lambda_sigma)
generate_model_normal_distribution <- function(k_indicators, lambda_mu, lambda_sigma) {
  # Ensure that k_indicators is an integer and coefficient is numeric
  if (!is.numeric(k_indicators) || !is.numeric(lambda_mu) || !is.numeric(lambda_sigma)) {
    stop("k_indicators, lambda_mu, and lambda_sigma must be numeric")
  }
  
  # Generate each item string
  itemStrings <- sapply(1:k_indicators, function(i) {
    paste(format(rnorm(n = 1, mean = lambda_mu, sd = lambda_sigma), nsmall = 2), "*item", i, sep = "")
  })
  
  # Concatenate all item strings with " + " separator
  resultString <- paste("factor =~", paste(itemStrings, collapse = " + "))
  
  return(resultString)
}

remove_worst_items <- function(data, k_indicators_retained) {
  
  k_indicators <- ncol(data)
  k_indicators_to_remove <- k_indicators - k_indicators_retained
  
  for (k in 1:k_indicators_to_remove) {
    if (k_indicators <= 2) {
      warning("Cannot remove more items without violating minimum required for scale reliability analysis.")
      break
    }
    
    alphas_if_removed <- sapply(1:k_indicators, function(item) {
      suppressMessages(alpha(data[, -item], warnings = FALSE)$total$raw_alpha)
    })
    
    item_to_remove <- which.max(alphas_if_removed)
    data <- data[, -item_to_remove, drop = FALSE]
    k_indicators <- ncol(data)
  }
  
  return(data)
}


# define results vectors
alpha_change <- numeric(n_iterations) 

# run simulation
for (i in 1:n_iterations) {
  # generate data using the specified model
  data <- simulateData(generate_model_normal_distribution(k_indicators = k_indicators, 
                                                          lambda_mu = lambda_mu, 
                                                          lambda_sigma = lambda_sigma), 
                       sample.nobs = n_participants)
  
  # compute Cronbach's alpha for the full scale
  alpha_initial <- suppressMessages(alpha(data, warnings = FALSE)$total$raw_alpha)
  
  # drop K worst performing items based on iterative application of max-alpha-if-removed
  data_reduced <- remove_worst_items(data = data, k_indicators_retained = k_indicators_retained)
  
  # final alpha for shortened scale
  alpha_final <- suppressMessages(alpha(data_reduced, warnings = FALSE)$total$raw_alpha)
  
  # save results for iteration
  alpha_change[i] <- alpha_final - alpha_initial
}

# results
hist(alpha_change)
median(alpha_change)
#hist(alpha_change[alpha_change > 0])
#median(alpha_change[alpha_change > 0])

```

why is alpha lower rather than higher? because it trims so many items?

# manipulate all settings at once

```{r}

library(lavaan)  # For simulation of data
library(psych)   # For Cronbach's alpha calculation

set.seed(42) 

# Parameters to iterate over
k_indicators_values <- c(10, 20, 30)
k_indicators_retained_values <- c(3, 5, 10)
lambda_mu_values <- seq(0.3, 0.8, by = 0.1)
lambda_sigma <- 0.058
n_iterations <- 1 

# Initialize results list
results <- list()

# Simulation counter
simulation_counter <- 1

for (k_indicators in k_indicators_values) {
  for (n_participants_multiplier in c(3, 10)) {
    n_participants <- n_participants_multiplier * k_indicators
    for (k_indicators_retained in k_indicators_retained_values) {
      for (lambda_mu in lambda_mu_values) {
        for (i in 1:n_iterations) {
          data <- simulateData(generate_model_normal_distribution(k_indicators = k_indicators, 
                                                                  lambda_mu = lambda_mu, 
                                                                  lambda_sigma = lambda_sigma), 
                               sample.nobs = n_participants)
          
          alpha_initial <- suppressMessages(alpha(data, warnings = FALSE)$total$raw_alpha)
          
          data_reduced <- remove_worst_items(data = data, k_indicators_retained = k_indicators_retained)
          
          alpha_final <- suppressMessages(alpha(data_reduced, warnings = FALSE)$total$raw_alpha)
          
          alpha_change <- alpha_final - alpha_initial
          
          # Save results for iteration
          results[[simulation_counter]] <- list(
            k_indicators = k_indicators,
            n_participants = n_participants,
            k_indicators_retained = k_indicators_retained,
            lambda_mu = lambda_mu,
            alpha_change = alpha_change
          )
          simulation_counter <- simulation_counter + 1
        }
      }
    }
  }
}

# Convert the list of results to a data frame for easier analysis and visualization
results_df <- do.call(rbind, lapply(results, function(x) data.frame(t(unlist(x)))))

```

# manipulate all settings at once, using apply() rather than for loops

```{r}

library(lavaan)
library(psych)
library(parallel)
library(tidyr)
library(dplyr)
library(readr)

set.seed(42)

# Define simulation parameters

# ## single set of parameters, for testing
# k_indicators_values <- 10
# n_participants_multiplier_values <- 3
# k_indicators_retained_values <- 3
# lambda_mu_values <- 0.3
# lambda_sigma_values <- 0.058
# n_iterations <- 2

## factorial design
k_indicators_values <- c(10, 20, 30)
n_participants_multiplier_values <- c(3, 10)
k_indicators_retained_values <- c(3, 5, 10)
lambda_mu_values <- seq(0.3, 0.8, by = 0.1)
lambda_sigma_values <- 0.058
n_iterations <- 2

# Create a data frame of all parameter combinations
param_grid <- expand.grid(
  k_indicators = k_indicators_values,
  n_participants_multiplier = n_participants_multiplier_values,
  k_indicators_retained = k_indicators_retained_values,
  lambda_mu = lambda_mu_values,
  lambda_sigma = lambda_sigma_values,
  iteration = 1:n_iterations
)

# Function to perform a single simulation iteration
run_simulation <- function(k_indicators, n_participants_multiplier, k_indicators_retained, lambda_mu, lambda_sigma) {
  
  n_participants <- n_participants_multiplier * k_indicators
  
  data <- simulateData(generate_model_normal_distribution(k_indicators = k_indicators, 
                                                          lambda_mu = lambda_mu, 
                                                          lambda_sigma = lambda_sigma), 
                       sample.nobs = n_participants)
  
  alpha_initial <- alpha(data, warnings = FALSE)$total$raw_alpha
  
  data_reduced <- remove_worst_items(data = data, k_indicators_retained = k_indicators_retained)
  
  alpha_final <- alpha(data_reduced, warnings = FALSE)$total$raw_alpha
  
  alpha_change <- alpha_final - alpha_initial
  
  return(list(iteration = i,
              k_indicators = k_indicators,
              n_participants = n_participants,
              k_indicators_retained = k_indicators_retained,
              lambda_mu = lambda_mu,
              lambda_sigma = lambda_sigma,
              alpha_change = alpha_change))
}

# single run of function
results <- run_simulation(k_indicators = 10,
                          n_participants_multiplier = 3,
                          k_indicators_retained = 5,
                          lambda_mu = 0.40,
                          lambda_sigma = 0.058) |>
  suppressMessages()

# # Use mapply to iterate over the parameter grid
# results <- mapply(FUN = run_simulation,
#                   k_indicators = param_grid$k_indicators,
#                   n_participants_multiplier = param_grid$n_participants_multiplier,
#                   k_indicators_retained = param_grid$k_indicators_retained,
#                   lambda_mu = param_grid$lambda_mu,
#                   lambda_sigma = param_grid$lambda_sigma,
#                   SIMPLIFY = FALSE) |>
#   suppressMessages() |>
#   # convert results to a data frame
#   bind_rows()

# Example adapted for mcmapply - run in parallel
results <- mcmapply(FUN = run_simulation, 
                    k_indicators = param_grid$k_indicators, 
                    n_participants_multiplier = param_grid$n_participants_multiplier, 
                    k_indicators_retained = param_grid$k_indicators_retained, 
                    lambda_mu = param_grid$lambda_mu, 
                    lambda_sigma = param_grid$lambda_sigma,
                    SIMPLIFY = FALSE, 
                    mc.cores = detectCores() - 1) |> # use one less than the total number of cores so your system remains useable for other tasks
  suppressMessages() |>
  # convert results to a data frame
  bind_rows() 

# save to disk
write_rds(results, "results.rds")

# summarize across iterations 
results %>%
  group_by(k_indicators, n_participants, k_indicators_retained, lambda_mu) |>
  summarise(mean_alpha_change = mean(alpha_change, na.rm = TRUE)) |>
  # Reshaping for clearer comparison across lambda_mu values
  pivot_wider(names_from = lambda_mu, 
              values_from = mean_alpha_change,
              names_prefix = "lambda_mu_") |>
  mutate_if(is.numeric, janitor::round_half_up, digits = 2)

```

# manipulate all settings at once, using tidy workflow

not working but close! see TODO below

```{r}

library(lavaan)
library(psych)
library(parallel)
library(tidyr)
library(dplyr)
library(readr)

set.seed(42)

# generate data ----
generate_data <- function(k_indicators, n_participants_multiplier, lambda_mu, lambda_sigma) {
  
  n_participants <- n_participants_multiplier * k_indicators
  
  generate_model_normal_distribution <- function(k_indicators, lambda_mu, lambda_sigma) {
    # ensure that k_indicators is an integer and coefficient is numeric
    if (!is.numeric(k_indicators) || !is.numeric(lambda_mu) || !is.numeric(lambda_sigma)) {
      stop("k_indicators, lambda_mu, and lambda_sigma must be numeric")
    }
    
    # generate each item string
    itemStrings <- sapply(1:k_indicators, function(i) {
      paste(format(rnorm(n = 1, mean = lambda_mu, sd = lambda_sigma), nsmall = 2), "*item", i, sep = "")
    })
    
    # concatenate all item strings with " + " separator
    resultString <- paste("factor =~", paste(itemStrings, collapse = " + "))
    
    return(resultString)
  }
  # # example usage
  # generate_model_normal_distribution(k_indicators = 5, lambda_mu = 0.4, lambda_sigma = 0.058)
  
  data <- simulateData(generate_model_normal_distribution(k_indicators = k_indicators, 
                                                          lambda_mu = lambda_mu, 
                                                          lambda_sigma = lambda_sigma), 
                       sample.nobs = n_participants)
  
  return(data)
}

# analyse data ----
analyze_data <- function(data, k_indicators_retained) {
  
  alpha_initial <- psych::alpha(data, warnings = FALSE)$total$raw_alpha
  
  remove_worst_items <- function(data, k_indicators_retained) { # \TODO: move the remove_worst_items function to outside analyze data, make it a hack function. Then apply the same calculate alpha function to both the original and hacked data, saving separately. calculate delta after unnesting.
    
    k_indicators <- ncol(data)
    k_indicators_to_remove <- k_indicators - k_indicators_retained
    
    for (k in 1:k_indicators_to_remove) {
      if (k_indicators <= 2) {
        warning("Cannot remove more items without violating minimum required for scale reliability analysis.")
        break
      }
      
      alphas_if_removed <- sapply(1:k_indicators, function(item) {
        suppressMessages(alpha(data[, -item], warnings = FALSE)$total$raw_alpha)
      })
      
      item_to_remove <- which.max(alphas_if_removed)
      data <- data[, -item_to_remove, drop = FALSE]
      k_indicators <- ncol(data)
    }
    return(data)
  }
  
  data_reduced <- remove_worst_items(data = data, k_indicators_retained = k_indicators_retained)
  
  alpha_final <- psych::alpha(data_reduced, warnings = FALSE)$total$raw_alpha
  
  alpha_change <- alpha_final - alpha_initial
  
  return(tibble(alpha_initial = alpha_initial,
                alpha_final = alpha_final,
                alpha_change = alpha_change))
}

# define parameters ----
experiment_parameters_grid <- expand.grid(
  k_indicators = c(10, 20, 30),
  n_participants_multiplier = c(3, 10),
  lambda_mu = seq(0.3, 0.8, by = 0.1),
  lambda_sigma = 0.058,
  k_indicators_retained = c(3, 5, 10),
  iteration = 1:1
)

# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(k_indicators, 
                                    n_participants_multiplier, 
                                    lambda_mu, 
                                    lambda_sigma),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data,
                                      k_indicators_retained),
                                 analyse_data))

# # summarize across iterations 
# results %>%
#   group_by(k_indicators, n_participants, k_indicators_retained, lambda_mu) |>
#   summarise(mean_alpha_change = mean(alpha_change, na.rm = TRUE)) |>
#   # Reshaping for clearer comparison across lambda_mu values
#   pivot_wider(names_from = lambda_mu, 
#               values_from = mean_alpha_change,
#               names_prefix = "lambda_mu_") |>
#   mutate_if(is.numeric, janitor::round_half_up, digits = 2)

```

