---
title: "Assessing the statistical power of assumption tests"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Illustrate skew

```{r}

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 0) |>
  hist(main = "Skew-normal data when alpha is large (0)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 1) |>
  hist(main = "Skew-normal data when alpha is large (1)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 2) |>
  hist(main = "Skew-normal data when alpha is large (2)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 3) |>
  hist(main = "Skew-normal data when alpha is large (3)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 6) |>
  hist(main = "Skew-normal data when alpha is large (6)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 9) |>
  hist(main = "Skew-normal data when alpha is large (9)", xlab = "Score")

rsn(n = 100000, 
    xi = 0, 
    omega = 1, 
    alpha = 12) |>
  hist(main = "Skew-normal data when alpha is large (6)", xlab = "Score")

```

# Hypotheis testing with skew normal data and Welches' t test

```{r fig.height=5, fig.width=10}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)
library(rsn)

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n_control,
                          n_intervention,
                          location_control, # location, akin to mean
                          location_intervention,
                          scale_control, # scale, akin to SD
                          scale_intervention,
                          skew_control, # slant/skew. When 0, produces normal/gaussian data
                          skew_intervention) {
  
  data_control <- 
    tibble(condition = "control",
           score = rsn(n = n_control, 
                       xi = location_control, # location, akin to mean
                       omega = scale_control, # scale, akin to SD
                       alpha = skew_control)) # slant/skew. When 0, produces normal/gaussian data
  
  data_intervention <- 
    tibble(condition = "intervention",
           score = rsn(n = n_intervention, 
                       xi = location_intervention, # location, akin to mean
                       omega = scale_intervention, # scale, akin to SD
                       alpha = skew_intervention)) # slant/skew. When 0, produces normal/gaussian data
  
  data <- bind_rows(data_control,
                    data_intervention) 
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  # dependencies
  require(effsize)
  
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = FALSE,
                       alternative = "two.sided")
  
  res_cohens_d <- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d
                                   within = FALSE,
                                   data = data)
  
  res <- tibble(p = res_t_test$p.value, 
                cohens_d = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble
                cohens_d_ci_lower = res_cohens_d$conf.int["lower"],
                cohens_d_ci_upper = res_cohens_d$conf.int["upper"])
  
  return(res)
}


# define experiment parameters ----
experiment_parameters_grid_a <- expand_grid(
  n_control = 100,
  n_intervention = 100,
  location_control = 0,
  location_intervention = 0, 
  scale_control = 1,
  scale_intervention = 1,
  skew_control = 0,
  skew_intervention = 0, 
  iteration = 1:1 
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n_control,
                                    n_intervention,
                                    location_control,
                                    location_intervention,
                                    scale_control,
                                    scale_intervention,
                                    skew_control,
                                    skew_intervention),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  

# # optional: save results to disk
# write_rds(simulation, "simulation_1_results.rds")
# # read from disk in future if it exists already
# simulation <- read_rds("simulation_1_results.rds")


# # summarise simulation results over the iterations ----
# ## estimate power  
# ## ie what proportion of p values are significant (< .05)
# simulation_summary <- simulation |>
#   unnest(analysis_results) |>
#   mutate(n_control = as.factor(n_control),
#          n_intervention = as.factor(n_intervention),
#          true_effect = paste("Cohen's d =", mean_intervention)) |>
#   group_by(n_control,
#            n_intervention,
#            true_effect) |>
#   summarize(power = mean(p < .05), 
#             mean_cohens_d_precision = mean((cohens_d_ci_upper - cohens_d_ci_lower)/2),
#             .groups = "drop")
# 
# # plot power
# ggplot(simulation_summary, aes(n_control, power, fill = n_intervention)) +
#   geom_col(position = position_dodge(width = 0.4), width = 0.4) +
#   scale_fill_viridis_d(option = "mako", begin = 0.3, end = 0.8, 
#                        guide = guide_legend(reverse = TRUE)) +
#   theme_linedraw() +
#   ggtitle("All conditions") +
#   ylab("Estimated statistical power") +
#   facet_wrap(~ true_effect)
# 
# # plot precision
# ggplot(simulation_summary, aes(n_control, mean_cohens_d_precision, fill = n_intervention)) +
#   geom_col(position = position_dodge(width = 0.4), width = 0.4) +
#   scale_fill_viridis_d(option = "mako", begin = 0.3, end = 0.8, 
#                        guide = guide_legend(reverse = TRUE)) +
#   theme_linedraw() +
#   ggtitle("All conditions") +
#   ylab("Precision of Cohen's d\n(mean half-95% CI width)") +
#   facet_wrap(~ true_effect)

```

# Power of Shapiro-Wilk's test with skew normal data

## In a single samples

```{r fig.height=5, fig.width=10}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(sn)
library(knitr)
library(kableExtra)

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n,
                          location, # location, akin to mean
                          scale, # scale, akin to SD
                          skew) { # slant/skew. When 0, produces normal/gaussian data
  
  data <- 
    tibble(score = rsn(n = n, 
                       xi = location, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  fit <- shapiro.test(data$score)
  results <- tibble(p = fit$p.value)
  
  return(results)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n = seq(from = 10, to = 100, by = 10),
  location = 0, # location, akin to mean
  scale = 1,    # scale, akin to SD
  skew = c(0, 1, 2, 3, 6, 9, 12),     # slant/skew. When 0, produces normal/gaussian data
  iteration = 1:1000
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n,
                                    location,
                                    scale,
                                    skew),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  
# summarise simulation results over the iterations ----
## ie what proportion of p values are significant (< .05)
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  mutate(n = as.factor(n)) |>
  group_by(n, 
           location, 
           scale, 
           skew) |>
  summarize(proportion_of_significant_results = mean(p < .05),
            .groups = "drop")

simulation_summary |>
  filter(proportion_of_significant_results >= .95) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## In either of two samples

```{r fig.height=5, fig.width=10}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(sn)
library(knitr)
library(kableExtra)

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n,
                          location, # location, akin to mean
                          scale, # scale, akin to SD
                          skew) { # slant/skew. When 0, produces normal/gaussian data
                          
  
  data_control <- 
    tibble(condition = "control",
           score = rsn(n = n, 
                       xi = location, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  data_intervention <- 
    tibble(condition = "intervention",
           score = rsn(n = n, 
                       xi = location, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  data <- bind_rows(data_control,
                    data_intervention) 
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  fit_intervention <- shapiro.test(data$score[data$condition == "intervention"])
  fit_control <- shapiro.test(data$score[data$condition == "control"])

  results <- tibble(p_intervention = fit_intervention$p.value, 
                    p_control = fit_control$p.value) 
  
  return(results)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n = seq(from = 10, to = 100, by = 10), # n per condition, not total
  location = 0, # location, akin to mean
  scale = 1,    # scale, akin to SD
  skew = c(0, 1, 2, 3, 6, 9, 12),     # slant/skew. When 0, produces normal/gaussian data
  iteration = 1:1000
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n,
                                    location,
                                    scale,
                                    skew),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  
# summarise simulation results over the iterations ----
## ie what proportion of p values are significant (< .05)
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  mutate(n = as.factor(n),
         lower_p = ifelse(p_intervention < p_control, p_intervention, p_control)) |>
  group_by(n, 
           location, 
           scale, 
           skew) |>
  summarize(proportion_of_significant_results = mean(lower_p < .05),
            .groups = "drop")

simulation_summary |>
  filter(proportion_of_significant_results >= .95) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Conditionally running a Welches' t-test or a Wilcoxon signed-rank test depending on Shapiro-Wilk's test for normality

```{r fig.height=5, fig.width=10}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(sn)
library(knitr)
library(kableExtra)

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n,
                          location_control, # location, akin to mean
                          location_intervention,
                          scale, # scale, akin to SD
                          skew) { # slant/skew. When 0, produces normal/gaussian data
                          
  
  data_control <- 
    tibble(condition = "control",
           score = rsn(n = n, 
                       xi = location_control, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  data_intervention <- 
    tibble(condition = "intervention",
           score = rsn(n = n, 
                       xi = location_intervention, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  data <- bind_rows(data_control,
                    data_intervention) 
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  assumption_test_intervention   <- shapiro.test(data$score[data$condition == "intervention"])
  assumption_test_control        <- shapiro.test(data$score[data$condition == "control"])

  hypothesis_test_welches_t      <- t.test(formula = score ~ condition, 
                                      data = data,
                                      var.equal = FALSE,
                                      alternative = "two.sided")
  
  hypothesis_test_mann_whitney_u <- wilcox.test(formula = score ~ condition, 
                                                data = data,
                                                alternative = "two.sided")

  results <- tibble(
    assumption_test_p_intervention = assumption_test_intervention$p.value, 
    assumption_test_p_control = assumption_test_control$p.value,
    hypothesis_test_p_welches_t = hypothesis_test_welches_t$p.value, 
    hypothesis_test_p_mann_whitney_u = hypothesis_test_mann_whitney_u$p.value
  ) |>
    mutate(hypothesis_test_p_conditional = ifelse(min(assumption_test_p_intervention, assumption_test_p_control) < .05, 
                                                  hypothesis_test_p_mann_whitney_u,
                                                  hypothesis_test_p_welches_t))
  
  return(results)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n = seq(from = 10, to = 100, by = 10), # n per condition, not total
  location_control = 0, # location, akin to mean
  location_intervention = 0.2,
  scale = 1,    # scale, akin to SD
  skew = c(0, 1, 2, 3, 6, 9, 12),     # slant/skew. When 0, produces normal/gaussian data
  iteration = 1:1000
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n,
                                    location_control,
                                    location_intervention,
                                    scale,
                                    skew),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  
# summarise simulation results over the iterations ----
## ie what proportion of p values are significant (< .05)
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  mutate(n_per_group = as.factor(n)) |>
  group_by(n_per_group, 
           location_control,
           location_intervention,
           scale, 
           skew) |>
  summarize(power_assumption_test = mean(assumption_test_p_intervention < .05 | assumption_test_p_control < .05),
            power_u = mean(hypothesis_test_p_mann_whitney_u < .05),
            power_t = mean(hypothesis_test_p_welches_t < .05),
            power_u = mean(hypothesis_test_p_mann_whitney_u < .05),
            power_conditional = mean(hypothesis_test_p_conditional < .05),
            .groups = "drop") |>
  mutate(conditional_better_than_t = power_conditional > power_t,
         conditional_better_than_u = power_conditional > power_u,
         u_better_than_t = power_u > power_t,
         conditional_much_better_than_t = (power_conditional - power_t) > .05,
         conditional_much_better_than_u = (power_conditional - power_u) > .05,
         u_much_better_than_t = (power_u - power_t) > .05)

simulation_summary |>
  arrange(skew, n_per_group) |>
  kable() |>
  kable_classic(full_width = FALSE)

simulation_summary |>
  arrange(skew, n_per_group) |>
  select(n_per_group, skew, power_t, power_u, power_conditional, conditional_better_than_t) |>
  mutate(power_diff_cond_t = power_conditional - power_t) |>
  filter(conditional_better_than_t == TRUE & 
           power_diff_cond_t >= 0.05) |>
  kable() |>
  kable_classic(full_width = FALSE)

simulation_summary |>
  arrange(skew, n_per_group) |>
  select(n_per_group, skew, power_t, power_u, power_conditional, conditional_better_than_u) |>
  mutate(power_diff_cond_u = power_conditional - power_u) |>
  # filter(conditional_better_than_u == TRUE & 
  #          power_diff_cond_u >= 0.05) |>
  kable() |>
  kable_classic(full_width = FALSE)

simulation_summary |>
  arrange(skew, n_per_group) |>
  select(n_per_group, skew, power_t, power_u, power_conditional, conditional_better_than_t) |>
  mutate(power_diff_u_t = power_u - power_t) |>
  #filter(power_diff_u_t <= -0.05) |>
  kable() |>
  kable_classic(full_width = FALSE)

simulation_summary |>
  summarize(percent_u_better_than_t = mean(u_better_than_t)*100,
            percent_conditional_better_than_t = mean(conditional_better_than_t)*100,
            percent_conditional_better_than_u = mean(conditional_better_than_u)*100) |>
  mutate_if(is.numeric, janitor::round_half_up, digits = 1) |>
  kable() |>
  kable_classic(full_width = FALSE)

simulation_summary |>
  summarize(percent_u_much_better_than_t = mean(u_much_better_than_t)*100,
            percent_conditional_much_better_than_t = mean(conditional_much_better_than_t)*100,
            percent_conditional_much_better_than_u = mean(conditional_much_better_than_u)*100) |>
  mutate_if(is.numeric, janitor::round_half_up, digits = 1) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

- Increases in statistical power for conditional use of tests is relatively small
- This assumes non-normality takes the forms of skew normality, but it could be other distributions
- This doesn't consider distortions in effect size due to non-normality, conditional calculation of Ruscio's A etc. 

# Power of Kolmogov-Smirnov test with skew normal data

```{r fig.height=5, fig.width=10}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(sn)
library(knitr)
library(kableExtra)

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n,
                          location, # location, akin to mean
                          scale, # scale, akin to SD
                          skew) { # slant/skew. When 0, produces normal/gaussian data
  
  data <- 
    tibble(score = rsn(n = n, 
                       xi = location, # location, akin to mean
                       omega = scale, # scale, akin to SD
                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  fit <- ks.test(data$score, "pnorm", mean = mean(data$score), sd = sd(data$score))
  results <- tibble(p = fit$p.value)
  
  return(results)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n = seq(from = 10, to = 100, by = 10),
  location = 0, # location, akin to mean
  scale = 1,    # scale, akin to SD
  skew = c(0, 1, 2, 3, 6, 9, 12),     # slant/skew. When 0, produces normal/gaussian data
  iteration = 1:1000
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n,
                                    location,
                                    scale,
                                    skew),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  
# summarise simulation results over the iterations ----
## ie what proportion of p values are significant (< .05)
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  mutate(n = as.factor(n)) |>
  group_by(n, 
           location, 
           scale, 
           skew) |>
  summarize(proportion_of_significant_results = mean(p < .05),
            .groups = "drop")

simulation_summary |>
  #filter(proportion_of_significant_results >= .8) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Testing homogeneity of variances

Levene test / Bartlett test

Levene: mean
Bartlett: median (default). Use this so that the test of homogeneity of variances does not itself rely on normality.

```{r}

library(car)
leveneTest(values ~ group, data = data)

```


# Session info

```{r}

sessionInfo()

```


