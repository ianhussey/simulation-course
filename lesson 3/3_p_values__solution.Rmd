---
title: "Understanding *p* values using a simulations [assignment]"
subtitle: "Using the example of Pearson's $r$ correlations"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Check your learning

-   What is the distribution of *p* values under the null hypothesis? What does this actually mean, with regard to what *p* values you should expect when the true effect is null (zero population effect)?

p values are uniformly distributed under the null hypothesis. A uniform distribution means that all variables in the range between 0 and 1 are just as likely to occur. This means that although the null hypothesis is true (population effect of zero), the p values will dance back and forth between 0 and 1. There is no specific pattern among the p values. Because p= .01, .02, .03, .04 are just as probable as .98, .67, .83, a statistically significant result is still found sometimes despite the true effect being zero. When the alpha level is set to 0.05 (5%), then in 5% of the cases the resulting p value should be statistically significant, even though it is from a population with a true effect size of zero.

-   What is the distribution of *p* values under the alternative hypothesis? What does this actually mean, with regard to what *p* values you should expect when the true effect is non-null (non-zero population effect)? What happens to *p*-values when the true population effect is large relative to small?

Under the alternative hypothesis, the distribution of p values is dependent on the true effect size (Cohen's d). The larger this effect size is, the higher the true positive rate (the chance of identifying the effect).

-   Can you draw a 2X2 'truth table', i.e., test result (significant vs non-significant) and the true effect (population effect zero vs non-zero) and fill in the four boxes with "true positives", "true negatives", "false positives", and "false negatives"?

|                     | Test significant             | Test non-significant        |
|---------------------|------------------------------|-----------------------------|
| Population non-zero | true positive                | false negative (beta error) |
| Population zero     | false positive (alpha error) | true negative               |

-   How do the statistical concepts of a test's "alpha value" and its "statistical power" (aka "1 - beta") relate to the concepts of true positives, true negatives, false positives, and false negatives?

A test's alpha value means the set value of the false positive rate. This is the percentage of p values that become statistically significant, even though the true population effect is zero. So when we set the alpha value to 0.05, 5% of the p values in a population with a true effect of zero still become statistically significant, although there is no effect in reality.

The statistical power is a concept which defines the probability of correctly identifying a effect when there is a true effect in the population. Its opponent is the beta error (false negative rate), which indicates the falsely rejection of a effect, even though there is a effect in the population. When the beta error is low (high statistical power), the possibility of correctly identifying a existing population effect is high. When the beta error is high (low statistical power), the chance of identifying a existing effect is low.

-   Other than the population effect being non-zero, what can give rise to an inflated rate of significant \*p\* values? There are at least three answers.

1. The violation of assumptions.
2. p-hacking.
3. Publication bias

# Apply your learning

The code in the below chunk is a copy of the final working simulation from "3_p\_values\_\_lesson.Rmd". Modify the code appropriately to create a new simulation that demonstrates the following:

**In the lesson, we demonstrated that statistical power increases when the true population effect size increases. Of course, we can't change the true population effect size, we can only observe it. So, what can we do to improve our chances of detecting a given true effect? Statistical power also increases with the sample size employed in a given study. Demonstrate that this is the case via simulation. Additionally, the concept of power applies to all frequentist tests, not just Welch's t-test. In your simulation, the data generation should be correlated data rather than a difference in means between two groups, and data analysis should be the statistical significance of a correlation test (`cor.test()`) rather than a t-test.**

To simulate correlated data, you can you MASS::mvrnorm() or faux::

See Lisa DeBruine's guide to both of them [here](https://debruine.github.io/data-sim-workshops/articles/faux.html#multivariate-normal). Note that getting your head around the correlation matrix you are simulating for takes a little thought, and you may need to practice making matrices of different sizes to understand how it works properly. *NB Simulating multivariate data is useful and important - I'm making this a part of this assignment so you have an opportunity to practice it.*

In this case, you only need to simulate a 2X2 matrix, i.e., two variables whose population correlations are either 0 (population effect of zero) or 0.3 (true population effect of medium size, according to Cohen's 1988 guidelines). You can set population means ($\mu$) to 0 and SDs ($\sigma$) to 1.

Simulate data for sample sizes ranging from N = 25 to N = 400 in steps of 25. Observe the change in the proportion of significant results between (a) the true population effect and (b) as sample size increases, in a table and a plot.

What do you expect to observe? See if you can guess just from the description of the simulation. Check that you're right using the simulation.

**Make sure that you make use of the advice from lesson 1, "general structure of a simulation", when developing your simulation below. Ie, don't attempt to directly modify the code below, but instead develop and test the individual components and then put them all together.**

## More complete solution with more parameters in the data generation function

```{r fig.height=4, fig.width=6}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)
library(faux)
library(kableExtra)


# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(123)


# define data generating function ----
# more complete solution, but more complicated than necessary
generate_data <- function(sample_size,
                          mean_variable_a,
                          mean_variable_b,
                          sd_variable_a,
                          sd_variable_b,
                          effect_size) {
  
  data <-
    faux::rnorm_multi(
      n = sample_size,
      mu = c(mean_variable_a, mean_variable_b),
      sd = c(sd_variable_a, sd_variable_b),
      r = matrix(c(1,           effect_size, 
                   effect_size,          1), ncol = 2),
      varnames = c("A", "B")
    )

  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  res_cor_test <- cor.test(x = data$A, 
                           y = data$B, 
                           data = data,
                           alternative = "two.sided",
                           method = "pearson")
  
  res <- tibble(p = res_cor_test$p.value,
                r_observed = res_cor_test$estimate)
  
  return(res)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  sample_size = seq(from = 25, to = 400, by = 25),
  mean_variable_a = 0,
  mean_variable_b = 0,
  sd_variable_a = 1,
  sd_variable_b = 1,
  effect_size = c(0, 0.3),
  iteration = 1:10000 
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(sample_size,
                                    mean_variable_a,
                                    mean_variable_b,
                                    sd_variable_a,
                                    sd_variable_b,
                                    effect_size),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))


simulation_reshaped <- simulation |>
  # convert `analysis_results` nested-data-frame column to regular columns in the df. in this case, the p value.
  unnest(analysis_results) %>%
  group_by(sample_size, effect_size) %>%
  summarize(proportion_significant = mean(p < 0.05), .groups = "drop")


# plot
plot <- ggplot(simulation_reshaped, aes(x = sample_size, y = proportion_significant, color = as.factor(effect_size))) +
  geom_hline(yintercept = 0.05, linetype = "dashed") +
  geom_line() +
  geom_point() +
  xlim(0, 400) +
  scale_color_viridis_d(begin = 0.3, end = 0.7, name = "Population correlation") +
  scale_y_continuous(breaks = c(0, 0.05, 0.25, 0.5, 0.75, 1.0)) +
  labs(title = "Long-Run Probability of Significant Results",
       x = "Sample Size", 
       y = "Probability of significant results") +
  theme_linedraw()

ggsave("plot.png", plot, width = 6, height = 4, units = "in")

plot

# table
print(simulation_reshaped) %>%
  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

## Simpler solution with fewer parameters in the data generation function

Note that the chunk options are set to not run this code on knit, as it produces the same results as the above, just a slightly different implementation.

```{r fig.height=4, fig.width=6, eval=FALSE, include=TRUE}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)
library(faux)
library(kableExtra)


# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(123)


# define data generating function ----
# more complete solution, but more complicated than necessary
generate_data <- function(sample_size,
                          effect_size) {
  
  data <-
    faux::rnorm_multi(
      n = sample_size,
      mu = c(0, 0),
      sd = c(1, 1),
      r = matrix(c(1,           effect_size, 
                   effect_size,          1), ncol = 2),
      varnames = c("A", "B")
    )

  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  res_cor_test <- cor.test(x = data$A, 
                           y = data$B, 
                           data = data,
                           alternative = "two.sided",
                           method = "pearson")
  
  res <- tibble(p = res_cor_test$p.value,
                r_observed = res_cor_test$estimate)
  
  return(res)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  sample_size = seq(from = 25, to = 400, by = 25),
  effect_size = c(0, 0.3),
  iteration = 1:10000 
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(sample_size,
                                    effect_size),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))


simulation_reshaped <- simulation |>
  # convert `analysis_results` nested-data-frame column to regular columns in the df. in this case, the p value.
  unnest(analysis_results) %>%
  group_by(sample_size, effect_size) %>%
  summarize(proportion_significant = mean(p < 0.05), .groups = "drop")


# plot
plot <- ggplot(simulation_reshaped, aes(x = sample_size, y = proportion_significant, color = as.factor(effect_size))) +
  geom_hline(yintercept = 0.05, linetype = "dashed") +
  geom_line() +
  geom_point() +
  xlim(0, 400) +
  scale_color_viridis_d(begin = 0.3, end = 0.7, name = "Population correlation") +
  scale_y_continuous(breaks = c(0, 0.05, 0.25, 0.5, 0.75, 1.0)) +
  labs(title = "Long-Run Probability of Significant Results",
       x = "Sample Size", 
       y = "Probability of significant results") +
  theme_linedraw()

# ggsave("plot.png", plot, width = 6, height = 4, units = "in")

plot

# table
print(simulation_reshaped) %>%
  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

# Session info

```{r}

sessionInfo()

```
