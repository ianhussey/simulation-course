---
title: "Standardized effect sizes and range restriction"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Citation & License

Citation: 

Ian Hussey (2024) Improving your statistical inferences through simulation studies in R. https://github.com/ianhussey/simulation-course

License: 

[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/deed.en)

```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies

```{r}

library(tidyverse)
library(scales)
library(sn)
library(janitor)
library(effsize)
library(knitr)
library(kableExtra)
library(faux)

```

# Load data

Real BDI-II data is taken from Cataldo et al. (2022) Abnormal Evidence Accumulation Underlies the Positive Memory Deficit in Depression, doi: [10.1037/xge0001268](https://doi.org/10.1037/xge0001268).

```{r}

data_bdi <- read_csv("data/bdi_data.csv")

```

# Why standardize? 

They have different possible ranges, different population means ($\mu$), and different population SDs ($\sigma$).

Even if had perfect that a given therapy has a (population) efficacy of lowering BDI-II depression scores by 6 points, without knowing a lot about the relationships between the BDI-II and other scores, we know little about how many points the same therapy would affect depression scores on the MADRS or the HAM-D. 

(surprisingly, very little work is ever done to collect information on the relationship between different scores so that we could know this)

Imagine three different published RCTs, each of which studied the efficacy of the same form of cognitive behavioral therapy for depression: 

- RCT 1 found that it lowered depression scores on the BDI-II by 6 points on average
- RCT 2 found that it lowered depression scores on the MADRS by 8 points on average
- RCT 3 found that it lowered depression scores on the HAM-D by 4 points on average

What is the efficacy of the intervention for depression scores on the PHQ-9? This is impossible to answer without knowing a lot about the details of the different scales (e.g., their min/max scores), the distribution of each scale's scores in the population (eg population $\mu$ and $\sigma$), and the relationship between different depression scales in the population. A one-point-change on one scale likely has a very different meaning to a one-point-change on another scale.

What is the efficacy of the intervention for depression *in general*? This too is impossible to answer as there is no common scale between them.

'Standardized' effect sizes are useful here as they provide common units. Instead of points on the self-report scale (i.e., sum scores), which differ between scales, standardized effect sizes generally use Standard Deviations as their units. For example, Cohen's d = 0.2 means that there are 0.2 Standard Deviations of difference between the two groups.

In principle, standardized effect sizes are extremely useful as they allow us to draw comparisons between studies using very different outcome measures, or indeed to synthesise results between such studies (i.e., meta-analysis). 

## Visualise

Semi-realistic depression scores on different scales.

```{r}

N <- 10000

generated_data <- 
  bind_rows(
    tibble(measure = "BDI-II",
           score = rsn(n = N, 
                       xi = 2,  # location
                       omega = 15, # scale
                       alpha = 16),
           max_score = 63), # skew
    tibble(measure = "HAM-D",
           score = rsn(n = N, 
                       xi = 33,  # location
                       omega = 7, # scale
                       alpha = -1),
           max_score = 52), # skew
    tibble(measure = "MADRS",
           score = rsn(n = N, 
                       xi = 7,  # location
                       omega = 7, # scale
                       alpha = 9),
           max_score = 60) # skew
  ) |>
  mutate(score = case_when(score < 0 ~ 0,
                           score > max_score ~ max_score,
                           TRUE ~ score))


ggplot(generated_data, aes(score)) +
  geom_vline(aes(xintercept = 0), linetype = "dotted") +
  geom_vline(aes(xintercept = max_score), linetype = "dotted") +
  geom_histogram(boundary = 0) +
  scale_x_continuous(breaks = breaks_pretty(n = 10)) +
  facet_wrap(~ measure, ncol = 1, scales = "free_y") +
  theme_linedraw() +
  ylab("Frequency") +
  xlab("Sum score")

```

For the moment, let's pretend like these scales produce continuous normal data that only differ in their population location ($\mu$) and scale ($\sigma$):

```{r}

generated_data <- 
  bind_rows(
    tibble(measure = "BDI-II",
           score = rnorm(n = N, mean = 7, sd = 9),
           max_score = 63),
    tibble(measure = "HAM-D",
           score = rnorm(n = N, mean = 12, sd = 4),
           max_score = 52),
    tibble(measure = "MADRS",
           score = rnorm(n = N, mean = 10, sd = 8),
           max_score = 60)
  ) 

ggplot(generated_data, aes(score)) +
  geom_histogram() +
  scale_x_continuous(breaks = breaks_pretty(n = 10)) +
  facet_wrap(~ measure, ncol = 1) +
  theme_linedraw() +
  ylab("Frequency") +
  xlab("Sum score")

```

A one-point change on the BDI-II still means something very different to a one-point change on the MADRS or HAM-D.

Data for a single sample can be standardized by taking each participant's score, deducting the mean score (the sample estimate of $\mu$), and then dividing by the SD of scores (the sample estimate of $\sigma$). Now, all scales have a mean of 0 and an SD of 1. A one-point change on any scale has the same interpretation: a one-standard deviation change on that scale's scores:

```{r}

generated_data <- 
  bind_rows(
    tibble(measure = "BDI-II",
           score = rnorm(n = N, mean = 0, sd = 1),
           max_score = 63),
    tibble(measure = "HAM-D",
           score = rnorm(n = N, mean = 0, sd = 1),
           max_score = 52),
    tibble(measure = "MADRS",
           score = rnorm(n = N, mean = 0, sd = 1),
           max_score = 60)
  ) 

ggplot(generated_data, aes(score)) +
  geom_histogram() +
  scale_x_continuous(breaks = breaks_pretty(n = 10)) +
  facet_wrap(~ measure, ncol = 1) +
  theme_linedraw() +
  ylab("Frequency") +
  xlab("Standaridized scores\n(score - mean)/SD")

```

Yay, now we have scores that can be compared between scales, e.g., in a meta-analysis. 

How can this go wrong?

# Influence of preselection on Cohen's d

Note that in the below, only data at pre is real BDI-II data. Data at post is modified data (i.e., offset by known amounts). 

## Example 1

### Wrangle/simulate

```{r}

set.seed(42)

subset_no_preselection <- data_bdi |>
  rename(control = bdi_score) |>
  # simulate a 'intervention' score that is 5 points lower than pre
  mutate(intervention = control - 5) |>
  # sample 100 participants from the real data 
  slice_sample(n = 100) |>
  mutate(recruitment = "General population") |>
  # reshape
  pivot_longer(cols = c(control, intervention),
               names_to = "condition",
               values_to = "bdi_score") |>
  mutate(condition = fct_relevel(condition, "control", "intervention"))


subset_preselection_for_severe <- data_bdi |>
  rename(control = bdi_score) |>
  # simulate recruitment into the study requiring a score of 29 or more at pre ("severe" depression according to the BDI-II manual)
  filter(control >= 29) |>
  # simulate a 'intervention' score that is 5 points lower than pre
  mutate(intervention = control - 5) |>
  # sample 100 participants from the real data 
  slice_sample(n = 100) |>
  mutate(recruitment = "'Severe' depression") |>
  # reshape
  pivot_longer(cols = c(control, intervention),
               names_to = "condition",
               values_to = "bdi_score") |>
  mutate(condition = fct_relevel(condition, "control", "intervention"))

```

### Plot

```{r}

bind_rows(subset_no_preselection,
          subset_preselection_for_severe) |>
  mutate(recruitment = fct_relevel(recruitment, "General population", "'Severe' depression")) |>
  ## plot
  ggplot(aes(bdi_score)) +
  geom_histogram(boundary = 0, bins = 21) +
  scale_fill_viridis_d(begin = 0.3, end = 0.7) +
  theme_linedraw() +
  coord_cartesian(xlim = c(-5, 63)) +
  facet_grid(condition ~ recruitment) +
  xlab("BDI-II sum score") +
  ylab("Frequency")

```

### Analyze

Exercise:

For each of the two datasets, please calculate:

- The unstandardized difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.  
- The standardized mean difference (Cohen's d) between the two groups (e.g., using `effsize::cohen.d()`).

Does the intervention work? Think about the simulated population effect.

```{r}

# datasets:
subset_no_preselection

subset_preselection_for_severe

```



Solution

```{r}

subset_no_preselection |>
  group_by(condition) |>
  summarize(mean_bdi_score = mean(bdi_score)) |>
  pivot_wider(names_from = condition,
              values_from = mean_bdi_score) |>
  mutate(mean_diff = intervention - control)

subset_preselection_for_severe |>
  group_by(condition) |>
  summarize(mean_bdi_score = mean(bdi_score)) |>
  pivot_wider(names_from = condition,
              values_from = mean_bdi_score) |>
  mutate(mean_diff = intervention - control)


effsize::cohen.d(formula = bdi_score ~ condition,
                 data = subset_no_preselection)$estimate |>
  round_half_up(2)

effsize::cohen.d(formula = bdi_score ~ condition,
                 data = subset_preselection_for_severe)$estimate |>
  round_half_up(2)

```

Equivalent change in means, different change in Cohen's d

We know for a fact that the true difference in means is the same in both studies, because we create the data to be this way (i.e., scores at post are exactly pre - 5). The unstandardized effect sizes (pre-post difference in means) are the same, by definition.

Despite this, the two studies produce the different Cohen's d values. The standardized effect sizes are the different, despite exactly the same pre-post differences between the studies. 

If the point of standardized effect sizes is to be able to compare them between studies on a common scale, and they don't do this, what is their point?

## Example 2

The only difference here is a) the true difference in means and b) the seed.

### Wrangle/simulate

```{r}

set.seed(46)

subset_no_preselection <- data_bdi |>
  rename(control = bdi_score) |>
  # simulate a 'intervention' score that is 5 points lower than pre
  mutate(intervention = control - 5) |>
  # sample 100 participants from the real data 
  slice_sample(n = 100) |>
  mutate(recruitment = "General population") |>
  # reshape
  pivot_longer(cols = c(control, intervention),
               names_to = "condition",
               values_to = "bdi_score") |>
  mutate(condition = fct_relevel(condition, "control", "intervention"))


subset_preselection_for_severe <- data_bdi |>
  rename(control = bdi_score) |>
  # simulate recruitment into the study requiring a score of 29 or more at pre ("severe" depression according to the BDI-II manual)
  filter(control >= 29) |>
  # simulate a 'intervention' score that is 5 points lower than pre
  mutate(intervention = control - 3) |>
  # sample 100 participants from the real data 
  slice_sample(n = 100) |>
  mutate(recruitment = "'Severe' depression") |>
  # reshape
  pivot_longer(cols = c(control, intervention),
               names_to = "condition",
               values_to = "bdi_score") |>
  mutate(condition = fct_relevel(condition, "control", "intervention"))

```

### Plot

```{r}

bind_rows(subset_no_preselection,
          subset_preselection_for_severe) |>
  mutate(recruitment = fct_relevel(recruitment, "General population", "'Severe' depression")) |>
  ## plot
  ggplot(aes(bdi_score)) +
  geom_histogram(boundary = 0, bins = 21) +
  scale_fill_viridis_d(begin = 0.3, end = 0.7) +
  theme_linedraw() +
  coord_cartesian(xlim = c(-5, 63)) +
  facet_grid(condition ~ recruitment) +
  xlab("BDI-II sum score") +
  ylab("Frequency")

```

### Analyze

Exercise:

Again, for each of the two datasets, please calculate:

- This is the unstandaridzied difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.  
- The standardized mean difference (Cohen's d) between the two groups (e.g., using `effsize::cohen.d()`).

Does the intervention work? Think about the simulated population effect.

```{r}

# datasets:
subset_no_preselection

subset_preselection_for_severe

```



Solution

```{r}

subset_no_preselection |>
  group_by(condition) |>
  summarize(mean_bdi_score = mean(bdi_score)) |>
  pivot_wider(names_from = condition,
              values_from = mean_bdi_score) |>
  mutate(mean_diff = intervention - control)

subset_preselection_for_severe |>
  group_by(condition) |>
  summarize(mean_bdi_score = mean(bdi_score)) |>
  pivot_wider(names_from = condition,
              values_from = mean_bdi_score) |>
  mutate(mean_diff = intervention - control)


effsize::cohen.d(formula = bdi_score ~ condition,
                 data = subset_no_preselection)$estimate |>
  round_half_up(2)

effsize::cohen.d(formula = bdi_score ~ condition,
                 data = subset_preselection_for_severe)$estimate |>
  round_half_up(2)

```

We know for a fact that the true difference in means is different, because we create the data to be this way (i.e., pre-post difference is -5 in the no preselection study and -3 in the severe depression preselection study). The unstandardized effect sizes (pre-post difference in means) are different, by definition.

Despite this, the two studies produce the same Cohen's d value. The standardized effect sizes are the same, despite genuine differences in the pre-post changes between the two studies. 

If the same standardized effect size estimate (Cohen's d) can represent different real changes in means, how can a Cohen's d of .2, for example, represent "small" effects? That is, if "small" effects on standardized effect sizes can represent unstandardized effect sizes of different sizes, how are standardized effect sizes 'standardized' at all? 


# Explanation 

The above results - where the same unstandarized effect sizes have different standardized effect sizes, or vice-versa - are due to the fact that standardized effect sizes involve dividing, in one way or another, unstandardized effect sizes by standard deviations. 

E.g., for Cohen's $d$:

$d = \frac{M_{intervention} - M_{control}}{SD_{pooled}}$

Most researchers are far more interested in the numerator than the denominator. 

- Researchers often care about how much the means differ between the intervention and control groups. Differences in the means determine whether the intervention 'worked' or not.
- They usually care very little about what the SD, except perhaps if they're assessing statistical assumptions (homogeneity of variances).

Despite this, the value of the SDs heavily influences the standardized effect size. 

In the above examples, the *range restriction* in the 'severe' depression condition produces a narrower range of scores, and therefore smaller smaller SDs. Dividing the same difference in means by a smaller value of SD produces a different Cohen's d estimate.

Range restrictions like these are extremely common in psychology research, where studies can differ in their inclusion/exclusion strategies. This means makes it far harder to compare 'standardized' effect sizes between studies than you might think.


## Standardized effect sizes require estimating multiple parameters

Cohen's d (usually) involves having to create a sample estimate of the means in each group. Researchers are usually more interested in differences between means. 

But it also involves having to estimate the SDs. This can be a little a little confusing the first time you encounter it: we often intuitively think of SD as the amount of noise around the signal we're interested in (the mean). We are somewhat more used to thinking about the fact that estimated means have error round them: the standard error of the mean (SEM) is used to calculate confidence intervals around means, and the SEM is actually just the SD of the mean (as opposed to normal SD, which is SD of the data). 

We are relatively less familiar with thinking about the fact that estimates of standard deviation also are estimated with error, e.g., the standard error of the SD, which is the SD of the SD. Confused yet?

We can understand this more easily with a simulation. We generate data for a single sample with a population mean ($\mu$) = 0 and population SD ($\sigma$) = 1.

Across lots of iterations, we can see that the average sample mean is close to the population mean ($\mu$), and the average sample SD is close to the population ($\sigma$):

```{r}

# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(123)


# define data generating function ----
generate_data <- function(n,
                          mean,
                          sd) {
  
  data <- tibble(score = rnorm(n = n, mean = mean, sd = sd))
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  
  res <- data |>
    summarize(sample_mean = mean(score),
              sample_sd = sd(score))
  
  return(res)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n = c(50, 100, 150),
  mean = 0,
  sd = 1,
  iteration = 1:1000
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n,
                                    mean,
                                    sd),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  

# summarise simulation results over the iterations ----
simulation_summary <- simulation |>
  unnest(analysis_results) 

simulation_summary |>
  group_by(n) |>
  summarize(average_sample_means = mean(sample_mean),
            average_sample_sds = mean(sample_sd)) |>
  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |>
  kable() |>
  kable_classic(full_width = FALSE)

```

But the estimated means in individual samples (i.e., individual iterations) vary around this true value ($\mu$ = 0). The smaller the sample size, the more deviation there is from the population value:

```{r fig.height=2.5, fig.width=6}

simulation_summary |>
  mutate(n_string = paste("N =", n),
         n_string = fct_relevel(n_string, "N = 50", "N = 100", "N = 150")) |>
  ggplot(aes(sample_mean)) +
  geom_histogram(boundary = 0) +
  theme_linedraw() +
  ylab("Frequency") +
  xlab("Means found in different samples\n(where population mu = 0)") +
  facet_wrap(~ n_string)

```

The same applies to the estimated SDs in individual samples (i.e., individual iterations), which also vary around this true value ($\sigma$ = 1). The smaller the sample size, the more deviation there is from the population value:

```{r fig.height=2.5, fig.width=6}

simulation_summary |>
  mutate(n_string = paste("N =", n),
         n_string = fct_relevel(n_string, "N = 50", "N = 100", "N = 150")) |>
  ggplot(aes(sample_sd)) +
  geom_histogram(boundary = 0) +
  theme_linedraw() +
  ylab("Frequency") +
  xlab("SDs found in different samples\n(where population sigma = 1)") +
  facet_wrap(~ n_string)

```

# Solutions to this problem

There are solutions to this, to make "standardized" effect sizes actually standard between studies. But almost no one does them.

1. The when calculating standardized effect sizes, use a well established population norm estimate of the measure's SD rather than the sample SD. E.g., always set the BDI's SD to 12 (or whatever your best estimate is). Note that no implementations of Cohen's d in commonly used R packages recommend this, and only a few can directly handle it (e.g., {esci}).
2. Use math/R packages to correct your standardised effect size estimate for *range restriction* (see Wiernik & Dahlke, 2020, doi: 10.1177/2515245919885611).

# Is this issue limited to Cohen's d?

No, it affects other forms of standardized effect sizes too, including correlations. 

E.g., there is a perennial debate in the US about whether standardized university entrance tests like the SAT are useful or not, or indeed are biased or not (e.g., between gender and race/ethnicity), because straightforward analyses suggest that SAT scores (used to get a place at university) are poorly predictive of grades at university. 

However, this poor predictive validity may be due in part to range restriction: because the SAT scores are used to determine who goes to university, data on university grades is only obtained from those individuals who already scored highly on the SAT. That is, there is a fairly narrow range of SAT scores among university students. Correlations, like Cohen's d, include SD in their denominator (i.e., $r = covariance_{xy}/(SD_x*SD_y)$), and therefore range restriction also distorts correlations. 

It is therefore possible - indeed, likely - that SAT scores are usefully predictive of grades at university. The below short simulation demonstrates attentuation in correlations due to range constraint. 

```{r fig.height=5, fig.width=5}

# Set seed for reproducibility
set.seed(42)

# Parameters
n <- 10000  # number of observations
rho <- 0.6  # correlation between x and y

# Generate correlated data using the faux package
simulated_data <- rnorm_multi(n = n, 
                              mu = c(0, 0), 
                              sd = c(1, 1), 
                              r = matrix(c(1, rho, 
                                           rho, 1), nrow = 2),
                              varnames = c("x", "y"))

# Calculate correlation in full data
full_correlation <- cor(simulated_data$x, simulated_data$y)
cat("Correlation in full data:", janitor::round_half_up(full_correlation, digits = 2), "\n")

# Introduce range restriction (e.g., keep only x > -0.5 and x < 0.5)
simulated_data_range_restricted <- simulated_data |>
  filter(x > qnorm(0.75)) # top 25% of a normal population corresponds to SD > qnorm(0.75), ie 0.6744898

# Calculate correlation in restricted data
restricted_correlation <- cor(simulated_data_range_restricted$x, simulated_data_range_restricted$y)
cat("Correlation in restricted data:", janitor::round_half_up(restricted_correlation, digits = 2), "\n")

# Plot full data with correlation annotation
ggplot(simulated_data, aes(x = x, y = y)) +
  geom_point(alpha = 0.4) +
  #geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggtitle("Correlation in Full Data") +
  theme_linedraw() +
  annotate("text", x = -2, y = 2, label = paste("r =", round(full_correlation, 2)), 
           hjust = 0.5, vjust = 0.5, size = 6, color = "blue") +
  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))

# Plot restricted data with correlation annotation
ggplot(simulated_data_range_restricted, aes(x = x, y = y)) +
  geom_point(alpha = 0.4) +
  #geom_smooth(method = "lm", se = FALSE, color = "red") +
  ggtitle("Correlation in Range Restricted Data") +
  theme_linedraw() +
  annotate("text", x = -2, y = 2, label = paste("r =", round(restricted_correlation, 2)), 
           hjust = 0.5, vjust = 0.5, size = 6, color = "red") +
  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))

```

Note that the observed correlations which have been distorted due to range restriction can be 'de-attentuated' or corrected if normative data is available to know what the unrestricted range looks like. However, this is very rarely done in studies and meta-analyses.

```{r}

# Calculate the variance ratios as an estimate of the range restriction factor
variance_ratio <- var(simulated_data_range_restricted$x) / var(simulated_data$x)

# Deattenuate the observed correlation
corrected_correlation <- restricted_correlation / sqrt(variance_ratio)

# Output results
cat("Observed Correlation (Restricted):", janitor::round_half_up(restricted_correlation, 2), "\n")
cat("Variance Ratio (Range Restriction Factor):", janitor::round_half_up(variance_ratio, 2), "\n")
cat("Corrected Correlation (Deattenuated):", janitor::round_half_up(corrected_correlation, 2), "\n")

```

Note that the corrected correlation is much closer to the original one.

```{r}

sessionInfo()

```


