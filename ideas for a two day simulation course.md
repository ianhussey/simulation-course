in a two day course, start by answering the "why simulate" and "what types of simulation" questions first. 

talk about the problem that people sometimes don't think about how they'll analyze data until they collect real data. reproducibility and research quality can be imporved by having any data, even junk, in order to write data processing and analysis scripts prior to data collection or as early as possible. 

this can simply be you giving nonsense responses a few times, and using that data to write data processing and analysis scripts.

better again, generate some pseudo plausible data by knowing what type of data the procedure produces (see: human created nonsense responses), understanding its structure, and generating uniform or normal data of arbitrary length following this.

if you can make pseudo plausible data, why not improve it more and simulate plausible data? 



too much research is like giving soldiers a gun for the first time on their way to the war zone. they should not be learning how to use it in a live fire environment, they should learn in controlled circumstances. becuase otherwise you naturally condition your analysis on the results; you treat the results as the stop signal for calling the analysis complete and correct. 

talk about the things you can simulate: true/false discovery rates, effect sizes, bias.
