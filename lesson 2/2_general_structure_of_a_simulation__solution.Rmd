---
title: "General structure of a simulation study [Assignment]"
subtitle: "Comparing the false-positive rate between Student's and Welches' independent t-tests"
author: "[student's name]"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Check your learning

- What are the five essential components of a simulation study?
- Can you explain what `tidyr::expand_grid()` does (in the context of a simulation)? 
- Can you explain what `purrr::pmap()` does (in the context of the simulation workflow above)? 
- Broadly speaking, what does changing the number of iterations used in a simulation do to the results of that simulation, and why? What analogy can be drawn with experiments with real participants?

# Apply your learning

The code in the below chunk is a copy of the final working simulation from "1_general_structure_of_a_simulation.Rmd". Modify the code appropriately to create a new simulation that demonstrates the following: 

*The Student's independent t-test assumes both equal variances (SDs) and equal sample sizes between groups. When this assumption is violated, it should suffer from an inflated false-positive rate (i.e., more than 5% of p values will be significant when alpha = .05). The Welches' t-test does not make this assumption: when the two groups have both different variances (SDs) and different Ns, the false-positive rate should remain at the alpha level. Demonstrate that this is the case via simulation.*

To do this, you should modify the simulation in the following ways. 

- Use the parameters listed in the below code chunk 'as-is': I've already modified it to contain the settings I want you to simulate for this question. Once you have a working simulation, you can also simulate for other parameters if you want to understand the conditions that determine when you find differences between the tests.
- Remove the Cohen's d calculation as we don't need it here.
- Compare the performance of the Student's independent t-test with a Welches' independent t-test. To do this, write a second data analysis function by copying and then modifying the existing one so that it saves the p value for a Welches' test instead.
- Both data analysis functions should be applied to same dataset, i.e., one generated dataset that is analysed in two different way. The p values of the two different tests should be saved as separate columns.
- To summarize the results of your new simulation, calculate the false positive rate (proportion of cases where p < .05) for each experimental condition.
- Try to also create histogram(s) that plot the distribution of p values for each of the two tests.

Remember that in both the previous simulation and this one you're studying the proportion of tests that produce a significant result. However, in the previous simulation the population effect size was non-zero, so the proportion of significant effects represents the tests statistical power (the true-positive rate). However, in this stimulation the population effect size is zero, so the proportion of significant effects represents the test's false-positive rate.

When modifying the code:

- Don't forget you can decrease the number of iterations during development and then increase it when you're ready to run the study for real. 
- Make modifications, do bug checking, and even make git commits incrementally. Think about not only getting the final simulation working, but the process of how you got there through making changes, checking that those changes produce the expected outcomes, and then moving on to the next task.
- If you're stuck, please post about it on Slack. I need feedback to determine whether this exercise is too easy or too hard for the group.

```{r}

# remove all objects from environment ----
rm(list = ls())


# dependencies ----
# repeated here for the sake of completeness 

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)


# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n_control,
                          n_intervention,
                          mean_control,
                          mean_intervention,
                          sd_control,
                          sd_intervention) {
  
  data <- 
    bind_rows(
      tibble(condition = "control",
             score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
      tibble(condition = "intervention",
             score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
    ) |>
    # control's factor levels must be ordered so that intervention is the first level and control is the second
    # this ensures that positive cohen's d values refer to intervention > control and not the other way around.
    mutate(condition = fct_relevel(condition, "intervention", "control"))
  
  return(data)
}


# define data analysis function ----
analyse_data_students <- function(data) {
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = TRUE,
                       alternative = "two.sided")
  
  res <- tibble(p_students = res_t_test$p.value)
  
  return(res)
}

# define data analysis function ----
analyse_data_welches <- function(data) {
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = FALSE,
                       alternative = "two.sided")
  
  res <- tibble(p_welches = res_t_test$p.value)

  return(res)
}


# define experiment parameters ----
experiment_parameters_grid <- expand_grid(
  n_control = 50,
  n_intervention = 25,
  mean_control = 0,
  mean_intervention = 0, 
  sd_control = 0.66,
  sd_intervention = 1.33,
  iteration = 1:5000 # increased number of iterations for more stable estimates. NB real stimulation are often much higher again.
)

# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n_control,
                                    n_intervention,
                                    mean_control,
                                    mean_intervention,
                                    sd_control,
                                    sd_intervention),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results_students = pmap(list(generated_data),
                                          analyse_data_students),
         analysis_results_welches = pmap(list(generated_data),
                                         analyse_data_welches))


# summarise simulation results over the iterations ----
simulation_reshaped <- simulation |>
  unnest(analysis_results_students) |>
  unnest(analysis_results_welches) |>
  pivot_longer(cols = c("p_students", "p_welches"),
               names_to = "test",
               values_to = "p") 

# table 
simulation_reshaped |>
  group_by(test) |>
  summarize(FPR = janitor::round_half_up(mean(p < .05), 2))

# plot
ggplot(simulation_reshaped, aes(p)) + 
  geom_histogram(boundary = 0, binwidth = 0.05) +
  geom_vline(xintercept = 0.05, color = "orange") +
  facet_wrap(~ test)

# plot zoomed in on p values between 0 and 0.10
ggplot(simulation_reshaped, aes(p)) + 
  geom_histogram(boundary = 0, binwidth = 0.01) +
  geom_vline(xintercept = 0.05, color = "orange") +
  facet_wrap(~ test) +
  coord_cartesian(xlim = c(0, 0.1))

```

# Conclusion and further reading

These simulations suggest that we should generally use the Welch's t-test rather than the Student's t-test, as the former is either equal to or better than the latter test across a range of assumptions either being met or broken. 

This point is is made and expanded on in a series of useful simulations by Delacre, Lakens & Leys (2017) "Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test" (see also their 2022 correction). To see a good example of a published simulation study that goes beyond what we've done here but makes substantively the same point, please read Delacre et al. (2017).

# Session info

```{r}

sessionInfo()

```


