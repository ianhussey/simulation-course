---
title: "General structure of a simulation study [Assignment]"
subtitle: "Comparing the false-positive rate between Student's and Welches' independent t-tests"
author: "[student's name]"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Check your learning

- What are the five essential components of a simulation study?
- Can you explain what `tidyr::expand_grid()` does (in the context of a simulation)? 
- Can you explain what `purrr::pmap()` does (in the context of the simulation workflow above)? 
- Broadly speaking, what does changing the number of iterations used in a simulation do to the results of that simulation, and why? What analogy can be drawn with experiments with real participants?

# Apply your learning

The code in the below chunk is a copy of the final working simulation from "1_general_structure_of_a_simulation.Rmd". Modify the code appropriately to create a new simulation that demonstrates the following: 

*The Student's independent t-test assumes both equal variances (SDs) and equal sample sizes between groups. When this assumption is violated, it should suffer from an inflated false-positive rate (i.e., more than 5% of p values will be significant when alpha = .05). The Welches' t-test does not make this assumption: when the two groups have both different variances (SDs) and different Ns, the false-positive rate should remain at the alpha level. Demonstrate that this is the case via simulation.*

To do this, you should modify the simulation in the following ways. 

- Use the parameters listed in the below code chunk 'as-is': I've already modified it to contain the settings I want you to simulate for this question. Once you have a working simulation, you can also simulate for other parameters if you want to understand the conditions that determine when you find differences between the tests.
- Remove the Cohen's d calculation as we don't need it here.
- Compare the performance of the Student's independent t-test with a Welches' independent t-test. To do this, write a second data analysis function by copying and then modifying the existing one so that it saves the p value for a Welches' test instead.
- Both data analysis functions should be applied to same dataset, i.e., one generated dataset that is analysed in two different way. The p values of the two different tests should be saved as separate columns.
- To summarize the results of your new simulation, calculate the false positive rate (proportion of cases where p < .05) for each experimental condition.
- Try to also create histogram(s) that plot the distribution of p values for each of the two tests.

Remember that in both the previous simulation and this one you're studying the proportion of tests that produce a significant result. However, in the previous simulation the population effect size was non-zero, so the proportion of significant effects represents the tests statistical power (the true-positive rate). However, in this stimulation the population effect size is zero, so the proportion of significant effects represents the test's false-positive rate.

When modifying the code:

- Don't forget you can decrease the number of iterations during development and then increase it when you're ready to run the study for real. 
- Make modifications, do bug checking, and even make git commits incrementally. Think about not only getting the final simulation working, but the process of how you got there through making changes, checking that those changes produce the expected outcomes, and then moving on to the next task.
- If you're stuck, please post about it on Slack. I need feedback to determine whether this exercise is too easy or too hard for the group.

```{r}

# # remove all objects from environment ----
# rm(list = ls())


# dependencies ----

library(tidyr)
library(dplyr)
library(forcats)
library(readr)
library(purrr) 
library(ggplot2)
library(effsize)


# set the seed ----
# for the pseudo random number generator to make results reproducible
set.seed(42)


# define data generating function ----
generate_data <- function(n_control,
                          n_intervention,
                          mean_control,
                          mean_intervention,
                          sd_control,
                          sd_intervention) {
  
  data <- 
    bind_rows(
      tibble(condition = "control",
             score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
      tibble(condition = "intervention",
             score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
    ) |>
    # control's factor levels must be ordered so that intervention is the first level and control is the second
    # this ensures that positive cohen's d values refer to intervention > control and not the other way around.
    mutate(condition = fct_relevel(condition, "intervention", "control"))
  
  return(data)
}


# define data analysis function ----
analyse_data <- function(data) {
  # dependencies
  require(effsize)
  
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = TRUE,
                       alternative = "two.sided")
  
  res_cohens_d <- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d
                                   within = FALSE,
                                   data = data)
  
  res <- tibble(p = res_t_test$p.value, 
                cohens_d = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble
                cohens_d_ci_lower = res_cohens_d$conf.int["lower"],
                cohens_d_ci_upper = res_cohens_d$conf.int["upper"])
  
  return(res)
}


# define experiment parameters ----
# note: these are the correct ones I want you to simulate. 
experiment_parameters_grid <- expand_grid(
  n_control = 50, # notice that the two sample sizes are different
  n_intervention = 25,
  mean_control = 0, # notice that the two means are both 0: the population difference in means is null.
  mean_intervention = 0,
  sd_control = 0.66, # notice that the two sample variances (via SDs) are different
  sd_intervention = 1.33,
  iteration = 1:5000 # the final simulation needs lots of p values for a good plot
)


# run simulation ----
simulation <- 
  # using the experiment parameters
  experiment_parameters_grid |>
  
  # generate data using the data generating function and the parameters relevant to data generation
  mutate(generated_data = pmap(list(n_control,
                                    n_intervention,
                                    mean_control,
                                    mean_intervention,
                                    sd_control,
                                    sd_intervention),
                               generate_data)) |>
  
  # apply the analysis function to the generated data using the parameters relevant to analysis
  mutate(analysis_results = pmap(list(generated_data),
                                 analyse_data))
  

# summarise simulation results over the iterations ----
## estimate power  
## ie what proportion of p values are significant (< .05)
simulation_summary <- simulation |>
  unnest(analysis_results) |>
  mutate(n_control = as.factor(n_control),
         n_intervention = as.factor(n_intervention),
         true_effect = paste("Cohen's d =", mean_intervention)) |>
  group_by(n_control,
           n_intervention,
           true_effect) |>
  summarize(power = mean(p < .05), 
            mean_cohens_d_precision = mean((cohens_d_ci_upper - cohens_d_ci_lower)/2),
            .groups = "drop")

```

# Session info

```{r}

sessionInfo()

```


